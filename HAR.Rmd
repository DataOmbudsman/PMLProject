---
title: "Human Activity Recognition"
author: "data-artist"
date: "Sunday, July 27, 2014"
---

# Executive summary

In this work, we construct predictive models for Human Activity Recognition. We aim to forecast how well the participants of an experiment are performing exercise activites. The data the we use are from accelerometers on their belts, forearms, arms, and dumbells. This is a classification problem that we intend to solve with Random Forests. Our results show that a nearly perfect classification of activity 

# Preparation

The caret and randomForest packages will be used later.

```{r libraries}
library(caret)
library(randomForest)
```

The files need to be downloaded. The data set was created by this research team: http://groupware.les.inf.puc-rio.br/har

```{r downloading-files, cache=TRUE}
trainURL <-
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

testURL <-
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(trainURL, "train.csv", method="curl")
download.file(testURL, "test.csv", method="curl")

trainData <- read.csv("train.csv")
testData <- read.csv("test.csv")
```

# Data cleaning

In the beginning, we have 160 columns in our data set. We check how many of the values according to each attribute is missing, and then remove those attributes that are lacking enough data. We remove 67 attributes in order to make modeling computationally easier.

```{r cleaning1}
pctMissing <- function(x) {
    sum(is.na(x)) / length(x)
    }

neededVector <- sapply(trainData, pctMissing) <= 0.7
trainData <- trainData[, neededVector] #removing 67

```

We also remove the timestamp data (3 columns) that we will not use for prediction. We also not need column #1 for modeling, which contains the row identificator.
```{r cleaning2}
# raw_timestamp_part_1, raw_timestamp_part_2 and cvtd_timestamp are in columns 1, 3, 4 and 5
trainData <- trainData[, -c(1, 3, 4, 5)]
```

Finally, we drop columns with names starting with "kurtosis", "skewness", "max_yaw", "min_yaw" and "amplitude" as they are clearly not filled with sufficient data (33 columns).

```{r cleaning3}
kurt <- grepl("^kurtosis_", names(trainData))
skew <- grepl("^skewness_", names(trainData))
miny <- grepl("^min_yaw_", names(trainData))
maxy <- grepl("^max_yaw_", names(trainData))
ampl <- grepl("^amplitude_", names(trainData))
combined <- kurt | skew | miny | maxy | ampl
combined <- ! combined

trainData <- trainData[, combined] #removing 33

```

In the end of data cleaning, we possess 55 columns as explanatory variables and one outcome variable.

# Model building ans results

## Algorithm

We choose to build Random Forests as they are among the state-of-the-art predictive models.

## Model #1: Variable Importance

In our first models, we use a Random Forest model with default settings. We call the original randomForest() function with certain parameters to be slightly more efficient. This modeling phase serves two functions:

- Observe variable importance in order to being able to pick the attributes having more predictive power later.
- Have a first estimation on out-of-sample error.

```{r model1, cache=TRUE}

set.seed(1234)

testIndex <- createDataPartition(trainData$classe,
                                p = 0.30, list=FALSE)
model1Train <- trainData[-testIndex, ]
model1Test <- trainData[testIndex, ]

model1 <- randomForest(classe ~ .,
                       data = model1Train,
                       proximity = FALSE,
                       mtry = 5,
                       sampsize = 2000,
                       maxnodes = 200)
```

According to this model, the importance list of variables is the following:

```{r variable-importance}
imp <- varImp(model1)
impOrder <- order(imp, decreasing = TRUE)
impOrdered <- as.data.frame(
    cbind(row.names(imp)[impOrder], imp[impOrder, ]))
print(impOrdered)
```

Seeing the decreasing predictive power of the varaibles, we decide to take the first 18 elements in this list in our final model.

Now let us take a look at the confusion matrix and accuracy of our first model. The accuracy on an out-of-sample set is about 95%.

```{r model1-pred}

model1Pred <- predict(model1, model1Test)

# confusion matrix
table(model1Pred, model1Test$classe)

# accuracy
sum(diag(table(model1Pred, model1Test$classe))) / nrow(model1Test)

```

## Model 2: 

The final model will use the 18 most important variables. We also use a 10-fold cross-validation during the training for fine-tuning the parameters of the Random Forest. As we have reduced the number of explanatory variables, the training time is reasonable with our machine.

```{r model2-preproc-modeling, cache=TRUE}

# keeping the most important variables
classe <- trainData$classe
trainData <- trainData[, impOrdered$V1[1:18]] 
trainData$classe <- classe

# new trainign and test set
set.seed(568)
testIndex <- createDataPartition(trainData$classe,
                                p = 0.30, list=FALSE)
mode2Train <- trainData[-testIndex, ]
mode2Test <- trainData[testIndex, ]

# parameters for training

control2 <- trainControl(method = "cv",
                         number = 10,
                         repeats = 1)

# train model

model2 <- train(classe ~ .,
                data = mode2Train,
                model = "rf",
                trControl = control2)

```

Finally, we estimate the out-of-sample prediction accuracy by applying the model onto an unseen set of data. We also obtain a confusion matrix of the classification.

```{r model2-pred}

model2Pred <- predict(model2, mode2Test)

# confusion matrix
table(model2Pred, mode2Test$classe)

# accuracy
sum(diag(table(model2Pred, mode2Test$classe))) / nrow(mode2Test)

```

The estimated accuracy is about 98%, which we interpret as a fair and fine result.
